
### Data gathering -  
### Data Preprocessing :

#### Exploration & Feature Engineering - 
1) Check Correlation using Heatmap.

2) Check Missing Values.

3) If categorical values are less than or equal to 2 perform Label encoding, else perform One Hot encoding.

4) The missing values were imputed by ‘Mean’.

5) Read test and train csv file using pandas.

6) Mapped the Target column with Y variable.

7) Dropped irrelevant features such as SK_ID_CURR.

8) The columns which are highly correlated with TARGET are generated by polynomial degree 2 & 3.

### Normalization - 

MinMaxscale has been used in this project to rescale the value of all the features in the range [0 to 1]


### Cross-validation - 

1) Perform cross validation and plot ROCAUC to check the performance  of the classifiers.

### Analysis - 

Here I used different classifiers  such as  Logistic regression, SGD classifier,  ExtraTree classifier, Random Forest , Voting Classifier, ExtraTree Classifier, MLPclassifier, XGBoost & AdaBoost.

### Observation -

1) Label Imbalance Issue.

2)Tune the weights by using class_weight method , by doing this we are adding extra weight to imbalanced class, which helps in increasing the performance  of the classifier.

3) The columns which are highly correlated with TARGET are generated by polynomial degree 2 & 3 which helps in improving the results.

### Libraries Used -

Numpy, Pandas, Scikit-learn, Seaborn, Matplotlib, Pylab & Scipy.

### NOTE - We tried various combination of classifiers to boost our prediction such as voting classifier with (Logistic regression, SGDClassifier, Random Forest) as an estimator, XGBoost, ExtraTree classifier & MLPClassifer but the best possible results are generated by Logistic regression i.e 0.73492


Algorithm
       Logistic Regression
Score
       0.73492
